# LogLens++

A production-ready streaming log analytics engine that transforms raw logs into structured metrics and insights. Built for developers and operations teams who need powerful log analysis without the complexity of enterprise solutions.

## Overview

LogLens++ processes log streams in real-time, extracting meaningful patterns, metrics, and insights from unstructured log data. It provides a declarative approach to metrics definition, SQL-style querying, and anomaly detectionâ€”all through a clean CLI interface and YAML configuration.

### Key Features

- **Streaming Ingestion**: Process logs line-by-line without loading entire files into memory
- **Dual Format Support**: Handles both JSON and plain text logs with auto-detection
- **Declarative Metrics**: Define metrics in YAML without code changes
- **Rolling Windows**: Sliding and tumbling window support for time-series analysis
- **SQL Query Interface**: Full SQL access to events and metrics (BI-friendly)
- **Anomaly Detection**: Automatic detection of spikes and drops with human-readable explanations
- **DuckDB Storage**: Fast, embedded analytical database optimized for time-series queries
- **Production-Ready CLI**: Professional command-line interface with rich output

## How LogLens++ Compares

| Feature | LogLens++ | Splunk | ELK Stack | CloudWatch |
|---------|-----------|--------|-----------|------------|
| **Setup time** | <5 min | Hours | Days | Vendor-managed |
| **Cost** | Free (OSS) | $$$ | $$ | $$ |
| **Scalability** | Single machine | Distributed | Distributed | Cloud-native |
| **Use case** | Development/Small ops | Enterprise | Medium-large | AWS ecosystem |
| **Deployment** | Embedded (single file) | Server-based | Cluster | SaaS |
| **Learning curve** | Low | Medium | High | Low |
| **SQL support** | âœ… Full SQL | Limited | Limited | Limited |
| **Real-time** | âœ… Streaming | âœ… | âœ… | âœ… |
| **Anomaly detection** | âœ… Built-in | âœ… (add-on) | âŒ (requires ML) | âœ… (Insights) |
| **Configuration** | YAML | GUI/Config | JSON/YAML | Web UI |
| **Data retention** | Configurable | Configurable | Configurable | Pay-per-GB |

### When to Use LogLens++

**Choose LogLens++ if you:**
- Need quick setup and immediate results
- Want zero-cost, open-source solution
- Prefer SQL for querying (BI-friendly)
- Need embedded deployment (no server setup)
- Want declarative metrics (YAML configuration)
- Are building internal tooling or prototypes
- Need lightweight solution for development/staging

**Consider alternatives if you:**
- Need distributed processing at scale (>100M events/day)
- Require enterprise features (RBAC, SSO, compliance)
- Need vendor support and SLAs
- Want managed service (CloudWatch, Datadog)
- Require advanced ML-based anomaly detection
- Need multi-tenant isolation

## Visual Examples

### Dashboard Visualization

LogLens++ includes a matplotlib-based dashboard for visualizing metrics and trends:

```bash
# Generate sample data and create dashboard
python examples/dashboard.py --generate-data

# This creates dashboard.png with:
# - Error rate trends over time
# - Events distribution by level (pie chart)
# - Top sources by event count
# - Error rate by source
# - Metric trends visualization
```

![Dashboard Example](examples/dashboard.png)

The dashboard provides a comprehensive view of your log analytics with multiple chart types optimized for different insights.

### Workflow Demonstration

See the complete workflow in action (ingestion â†’ query â†’ anomaly detection):

```bash
# Run the workflow demo
python examples/workflow_demo.py

# This demonstrates:
# 1. Ingestion: Reading and parsing log files
# 2. Metrics: Computing aggregations and storing results
# 3. Querying: SQL queries and time-bucketed analysis
# 4. Anomaly Detection: Identifying spikes and drops
```

The workflow demo can be recorded as a GIF or video to show the end-to-end process. See [examples/README.md](examples/README.md) for instructions on creating visual recordings.

## Architecture

LogLens++ follows a modular, pipeline-based architecture:

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Logs      â”‚â”€â”€â”€â”€â–¶â”‚  Ingestion   â”‚â”€â”€â”€â”€â–¶â”‚ Processing  â”‚â”€â”€â”€â”€â–¶â”‚   Storage    â”‚
â”‚  (Files)    â”‚     â”‚  (Parser)    â”‚     â”‚  (Windows)  â”‚     â”‚  (DuckDB)   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                                                    â”‚
                                                                    â–¼
                                                          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                                                          â”‚  Analytics   â”‚
                                                          â”‚ (Metrics +   â”‚
                                                          â”‚  Anomalies)  â”‚
                                                          â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Component Overview

- **`ingestion/`**: Line-by-line log parsing supporting JSON and plain text formats
- **`processing/`**: Rolling window processors for time-series aggregation
- **`storage/`**: DuckDB-based persistence with SQL query interface
- **`analytics/`**: Declarative metrics and anomaly detection
- **`cli/`**: Command-line interface for all operations
- **`utils/`**: Configuration management and shared utilities

## Design Tradeoffs

### Why DuckDB?

**Chosen for:**
- **Embedded Architecture**: No separate database server requiredâ€”runs in-process
- **Analytical Performance**: Columnar storage optimized for analytical queries
- **Time-Series Optimization**: Excellent performance on time-range queries
- **SQL Interface**: Full SQL support for BI tools and ad-hoc analysis
- **Zero Configuration**: Works out-of-the-box with a single file

**Tradeoffs:**
- **Concurrent Writes**: DuckDB is optimized for analytical workloads, not high-concurrency writes
- **File-Based**: Database is a single file (portable but not distributed)
- **Memory Usage**: Columnar format can be memory-intensive for very large datasets

**Alternative Considered**: SQLite
- SQLite is better for transactional workloads but slower for analytical queries
- DuckDB's columnar storage provides 10-100x better performance for aggregations

### Rolling Window Implementation

LogLens++ implements two window strategies:

#### Sliding Windows
- **Use Case**: Real-time monitoring, continuous metrics
- **Behavior**: Window continuously moves forward, removing old events as new ones arrive
- **Implementation**: Uses `deque` for O(1) append and O(1) removal of expired events
- **Memory**: Bounded by window size, not total event count

```python
# Window slides forward with each event
Window: [t-5m, t-4m, t-3m, t-2m, t-1m, t]  # Current
Event arrives â†’ Window: [t-4m, t-3m, t-2m, t-1m, t, t+1m]  # Slides
```

#### Tumbling Windows
- **Use Case**: Fixed intervals, batch processing, reporting
- **Behavior**: Fixed, non-overlapping time intervals
- **Implementation**: Aligns to time boundaries (hour/day boundaries)
- **Memory**: Clears after each window completes

```python
# Fixed intervals, no overlap
Window 1: [12:00, 12:05]
Window 2: [12:05, 12:10]
Window 3: [12:10, 12:15]
```

**Tradeoffs:**
- **Sliding**: More responsive but requires maintaining event buffer
- **Tumbling**: Lower memory but less granular for real-time monitoring

## Performance

Tested on M1 MacBook Pro, 16GB RAM:

- **Ingestion**: 50,000 events/second (JSON)
- **Query latency**: <100ms for 1M events
- **Memory usage**: ~200MB for 1M events (5min sliding window)
- **Database size**: ~50MB per 1M events (compressed)

### Performance Characteristics

**Ingestion Performance:**
- Throughput: 50,000 events/second (JSON format, single-threaded)
- Memory: O(window_size) per metric, bounded regardless of total events
- Latency: <10ms per event (excluding I/O)

**Query Performance:**
- Time-range queries: <100ms for 1M events (with indexes)
- Aggregations: Optimized by DuckDB's columnar storage
- SQL queries: Full query optimization by DuckDB

**Memory Efficiency:**
- Sliding windows: ~200MB for 1M events with 5-minute window
- Tumbling windows: Lower memory (clears after each window)
- Storage buffer: Managed efficiently by DuckDB

**Storage Efficiency:**
- Compression: ~50MB per 1M events (DuckDB columnar compression)
- Indexes: Minimal overhead, significant query speedup
- Scalability: Recommended <100GB per database file

## Installation

### Requirements

- Python 3.8+
- pip

### Install from Source

```bash
git clone https://github.com/shruthisenthilarasu/LogLens_Plus.git
cd LogLens_Plus/loglens_plus
pip install -e .
```

### Optional Dependencies

For dashboard visualizations:

```bash
pip install matplotlib
```

### Development Setup

```bash
pip install -e ".[dev]"
```

## Quick Start

### 1. Create Configuration

```bash
loglens config init
```

This creates `loglens.yaml` with default metrics and anomaly detection settings.

### 2. Ingest Logs

```bash
# JSON logs
loglens ingest app.log --format json

# Plain text logs
loglens ingest app.log --format text

# Auto-detect format
loglens ingest app.log
```

### 3. Query Metrics

```bash
# List all metrics
loglens metrics list

# View specific metric
loglens metrics show error_count

# SQL query
loglens query "SELECT * FROM events WHERE level = 'ERROR' LIMIT 10"
```

### 4. Detect Anomalies

```bash
# Check all metrics
loglens anomalies

# Check specific metric
loglens anomalies --metric error_count
```

## CLI Usage

### Commands

#### `loglens ingest <logfile>`

Ingest logs from a file into the database.

```bash
loglens ingest app.log
loglens ingest app.log --format json --source myapp
loglens ingest app.log --config production.yaml
```

**Options:**
- `--format, -f`: Log format (json/text/auto)
- `--source, -s`: Default source name
- `--db, -d`: Database file path
- `--config, -c`: Configuration file

#### `loglens metrics list`

List all stored metrics with counts and time ranges.

```bash
loglens metrics list
loglens metrics list --db production.db
```

#### `loglens metrics show <metric_name>`

Show recent values for a specific metric.

```bash
loglens metrics show error_count
loglens metrics show error_count --limit 20
```

#### `loglens query "<SQL query>"`

Execute SQL queries against the database.

```bash
# Simple query
loglens query "SELECT * FROM events LIMIT 10"

# Aggregation
loglens query "SELECT level, COUNT(*) FROM events GROUP BY level"

# JSON output
loglens query "SELECT * FROM metrics" --format json
```

#### `loglens anomalies`

Detect anomalies in stored metrics.

```bash
loglens anomalies
loglens anomalies --metric error_count --threshold 2.5
loglens anomalies --config production.yaml
```

**Options:**
- `--metric, -m`: Specific metric to check
- `--threshold, -t`: Z-score threshold (default: 2.0)
- `--window, -w`: Rolling window size (default: 20)
- `--config, -c`: Configuration file

#### `loglens stats`

Show database statistics.

```bash
loglens stats
loglens stats --hours 48
```

#### `loglens config init`

Create a default configuration file.

```bash
loglens config init
loglens config init --path myconfig.yaml
```

## Configuration

LogLens++ uses YAML configuration files to define metrics, time windows, and alert thresholds. This allows customization without code changesâ€”a production best practice.

### Example Configuration

```yaml
# Default values
default_source: myapp
default_level: INFO

# Storage
storage:
  db_path: loglens.db
  retention_days: 30

# Metrics
metrics:
  - name: error_count
    filter: event.level in ('ERROR', 'CRITICAL', 'FATAL')
    aggregation: count
    window: 5m
    description: Count of error-level events

  - name: events_by_source
    filter: 'True'
    aggregation: count
    window: 15m
    group_by: event.source

# Anomaly Detection
anomalies:
  - metric_name: error_count
    window_size: 20
    threshold: 2.0
    enabled: true
```

### Configuration Locations

LogLens++ searches for config files in this order:
1. Path specified with `--config` option
2. `loglens.yaml` in current directory
3. `loglens.yml` in current directory
4. `.loglens.yaml` in current directory
5. `~/.loglens.yaml` in home directory

See [examples/config_usage.md](examples/config_usage.md) for detailed configuration documentation.

## Example Workflows

### Workflow 1: Real-Time Error Monitoring

```bash
# 1. Set up monitoring config
loglens config init
# Edit loglens.yaml to define error_count metric

# 2. Ingest logs continuously (in a loop or via cron)
loglens ingest /var/log/app.log --config loglens.yaml

# 3. Check for anomalies
loglens anomalies --config loglens.yaml

# Output:
# ğŸš¨ error_count spiked 2.3x above baseline (45.00 vs 19.50 average)
```

### Workflow 2: BI Analysis

```bash
# 1. Ingest historical logs
loglens ingest historical.log --format json

# 2. Query error rates by hour
loglens query "
  SELECT 
    DATE_TRUNC('hour', timestamp) AS hour,
    COUNT(*) as total_events,
    COUNT(CASE WHEN level = 'ERROR' THEN 1 END) as error_count
  FROM events
  WHERE timestamp >= '2024-01-01'
  GROUP BY hour
  ORDER BY hour
"

# 3. Get top error sources
loglens query "
  SELECT source, COUNT(*) as error_count
  FROM events
  WHERE level = 'ERROR'
  GROUP BY source
  ORDER BY error_count DESC
  LIMIT 10
"
```

### Workflow 3: Custom Metrics Pipeline

```yaml
# config.yaml
metrics:
  - name: api_response_time
    filter: event.source == 'api' and 'response_time' in event.metadata
    aggregation: average
    window: 5m
    value_extractor: event.metadata.get('response_time', 0)
```

```bash
# Process logs with custom metrics
loglens ingest api.log --config config.yaml

# Query custom metrics
loglens metrics show api_response_time
```

## Programmatic Usage

### Python API

```python
from loglens.ingestion import LogIngestor
from loglens.storage import LogStorage
from loglens.analytics import Metric, MetricProcessor
from loglens.utils.config import load_config

# Load configuration
config = load_config('loglens.yaml')

# Create storage
storage = LogStorage('logs.db')

# Create ingestor
ingestor = LogIngestor()

# Process logs
for event in ingestor.ingest_file('app.log'):
    storage.insert_event(event)

# Compute metrics
metrics = config.to_metrics()
processor = MetricProcessor(metrics)

for event in ingestor.ingest_file('app.log'):
    updated = processor.add_event(event)
    for metric_name, result in updated.items():
        storage.insert_metric(
            metric_name=result.metric_name,
            window_start=result.window_start,
            window_end=result.window_end,
            value=result.value
        )
```

### Declarative Metrics

```python
from loglens.analytics import Metric, MetricProcessor

# Define metrics declaratively
metrics = [
    Metric(
        name='error_rate',
        filter=lambda e: e.level == 'ERROR',
        aggregation='count',
        window='5m'
    ),
    Metric(
        name='events_by_source',
        filter=lambda e: True,
        aggregation='count',
        window='15m',
        group_by=lambda e: e.source
    ),
]

processor = MetricProcessor(metrics)
# Metrics update automatically as events arrive
```

## How Rolling Windows Work

### Sliding Window Implementation

```python
from collections import deque
from datetime import datetime, timedelta

class SlidingWindow:
    def __init__(self, window_size):
        self.window_size = window_size
        self.events = deque()  # O(1) append/popleft
    
    def add_event(self, event):
        window_end = event.timestamp
        window_start = window_end - self.window_size
        
        # Add new event
        self.events.append(event)
        
        # Remove expired events (O(1) per event)
        while self.events and self.events[0].timestamp < window_start:
            self.events.popleft()
        
        # Compute metrics over current window
        return self.compute_metrics(window_start, window_end)
```

**Key Design Decisions:**
1. **`deque` over `list`**: O(1) removal from front vs O(n) for list
2. **Event-driven expiration**: Events removed as new ones arrive, not on timer
3. **Bounded memory**: Window size limits memory usage regardless of total events

### Tumbling Window Implementation

```python
class TumblingWindow:
    def __init__(self, window_size):
        self.window_size = window_size
        self.current_window_start = None
        self.current_window_end = None
        self.events = []
    
    def add_event(self, event):
        # Align to time boundaries
        if self.current_window_start is None:
            self.current_window_start = self.align_to_boundary(event.timestamp)
            self.current_window_end = self.current_window_start + self.window_size
        
        # Check if event belongs to current window
        if event.timestamp >= self.current_window_end:
            # Complete current window
            result = self.compute_metrics()
            self.events.clear()
            
            # Start new window
            self.current_window_start = self.current_window_end
            self.current_window_end = self.current_window_start + self.window_size
        
        self.events.append(event)
        return result
```

**Key Design Decisions:**
1. **Boundary alignment**: Windows align to clean time boundaries (hour/day)
2. **Automatic completion**: Windows complete when next event arrives
3. **Memory clearing**: Events cleared after window completion

## Performance

### Benchmarks

Tested on M1 MacBook Pro, 16GB RAM:

- **Ingestion**: 50,000 events/second (JSON)
- **Query latency**: <100ms for 1M events
- **Memory usage**: ~200MB for 1M events (5min sliding window)
- **Database size**: ~50MB per 1M events (compressed)

### Performance Characteristics

**Ingestion Performance:**
- Throughput: 50,000 events/second (JSON format, single-threaded)
- Memory: O(window_size) per metric, bounded regardless of total events
- Latency: <10ms per event (excluding I/O)

**Query Performance:**
- Time-range queries: <100ms for 1M events (with indexes)
- Aggregations: Optimized by DuckDB's columnar storage
- SQL queries: Full query optimization by DuckDB

**Memory Efficiency:**
- Sliding windows: ~200MB for 1M events with 5-minute window
- Tumbling windows: Lower memory (clears after each window)
- Storage buffer: Managed efficiently by DuckDB

**Storage Efficiency:**
- Compression: ~50MB per 1M events (DuckDB columnar compression)
- Indexes: Minimal overhead, significant query speedup
- Scalability: Recommended <100GB per database file

### Performance Considerations

**Optimization Tips:**
- **Batch Inserts**: Use `insert_events()` for multiple events
- **Indexes**: All time-based and filter columns are indexed automatically
- **Data Retention**: Regularly delete old data to maintain performance
- **Window Sizes**: Choose appropriate window sizes to balance memory and granularity
- **Query Patterns**: Use indexed columns (timestamp, level, source) for faster queries

## Project Structure

```
loglens_plus/
â”œâ”€â”€ loglens/
â”‚   â”œâ”€â”€ ingestion/      # Log parsing and ingestion
â”‚   â”œâ”€â”€ processing/      # Rolling window processors
â”‚   â”œâ”€â”€ storage/        # DuckDB storage layer
â”‚   â”œâ”€â”€ analytics/      # Metrics and anomaly detection
â”‚   â”œâ”€â”€ cli/            # Command-line interface
â”‚   â””â”€â”€ utils/          # Configuration and utilities
â”œâ”€â”€ examples/           # Example configs and scripts
â”œâ”€â”€ tests/              # Test suite
â”œâ”€â”€ README.md           # This file
â””â”€â”€ pyproject.toml      # Project configuration
```

## Development

### Running Tests

```bash
pytest
```

### Code Style

```bash
black loglens/
flake8 loglens/
```

### Adding New Features

1. Follow the modular architecture
2. Add configuration support for new features
3. Update CLI with new commands
4. Add examples and documentation

## License

MIT License

## Contributing

Contributions welcome! Please:
1. Follow the existing code style
2. Add tests for new features
3. Update documentation
4. Submit pull requests

## Acknowledgments

Built with:
- [DuckDB](https://duckdb.org/) - Analytical database
- [Typer](https://typer.tiangolo.com/) - CLI framework
- [Rich](https://rich.readthedocs.io/) - Terminal output
- [PyYAML](https://pyyaml.org/) - Configuration parsing

---

**LogLens++** - Making log analytics accessible, powerful, and production-ready.
